4. Pull Request Rejections
How to Implement:

Data Collection: Track the PRs that were closed without being merged or marked as rejected. This can be directly retrieved from Git APIs.
Weighting: If a PR is rejected or closed frequently, it might indicate lower proficiency, especially if the reason is poor code quality or failure to meet requirements. Weight users' proficiency lower based on frequent PR rejections.
Examine Feedback: If possible, analyze the code review feedback or rejection reasons to determine why a PR was rejected.
Tools to Use:

GitHub API: /repos/{owner}/{repo}/pulls?state=closed
Use a filter to check if the PR was merged or not.

# Example: Fetch rejected (closed but not merged) pull requests
prs = requests.get('https://api.github.com/repos/{owner}/{repo}/pulls?state=closed').json()
rejected_prs = [pr for pr in prs if pr['merged_at'] is None]

5. Type of Contribution
How to Implement:

Data Collection: Categorize PRs or commits based on whether they are for new features, bug fixes, refactoring, or documentation.
Weighting: New feature development or large refactoring projects are generally more complex and deserve higher weight in proficiency evaluation.
Methodology:
Branch Naming Conventions: Use naming conventions like feature/* for features, bugfix/* for bug fixes to categorize contributions.
Commit Message Parsing: Use keywords in commit messages to identify contribution types (e.g., “add”, “fix”, “refactor”).
Tools to Use:

GitHub, GitLab, or Bitbucket APIs to fetch commit and branch data.
Natural Language Processing (NLP) for commit message analysis.

6. Code Review Participation
How to Implement:

Data Collection: Track how often a user participates in reviewing others’ code. This can be done by fetching the number of reviews a user has submitted.
Weighting: Users who provide high-quality reviews, especially those that are accepted by the code author, show proficiency in the skill.
Automating: Pull data for the number of PRs a user has reviewed, along with the depth of feedback.
Tools to Use:

GitHub API: /repos/{owner}/{repo}/pulls/reviews
GitLab and Bitbucket APIs have similar endpoints to fetch code reviews.

# Example: Fetch reviews by a user
response = requests.get('https://api.github.com/repos/{owner}/{repo}/pulls/{pull_number}/reviews')
reviews = response.json()
for review in reviews:
    print(f"Review by: {review['user']['login']}, State: {review['state']}")

7. Language Breakdown
How to Implement:

Data Collection: Analyze the types of files a user contributes to (e.g., .py for Python, .js for JavaScript).
Language Detection: Use tools like GitHub’s Linguist library or manual file extension mapping to determine the programming languages.
Weighting: Users proficient in certain languages will have higher contributions in those languages. Use this to assign weight to a user's proficiency in each language.
Tools to Use:

Linguist or similar libraries to detect languages based on file extensions.

8. Issue Resolution and Commit Linkage
How to Implement:

Data Collection: Track the issues that a user resolves by linking commits or PRs to issues. You can link Jira, GitHub Issues, or GitLab Issues with commits or PRs.
Weighting: Solving more complex or high-priority issues should increase the user’s skill rating.
Tools to Use:

Jira API for issue-tracking.
GitHub API for linked issues: /repos/{owner}/{repo}/issues


9. Task Complexity
How to Implement:

Data Collection: Assign complexity scores to tasks (e.g., via Jira or other project management tools).
Weighting: More complex tasks should carry higher weight when assessing user proficiency.
Task Linkage: Track which users are assigned and complete these tasks, linking them to their commits or PRs.
Tools to Use:

Jira API or any project management tool that tracks task complexity and assignment.



